{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "from transformers import SwinForImageClassification\n",
    "from torchvision import transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŠ¸ë˜ì´ë‹ í˜•íƒœ / ê²€ì¦ í˜•íƒœì— ë”°ë¥¸ ì „ì²˜ë¦¬ ì •ì˜\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"microsoft/swin-base-patch4-window7-224\"\n",
    "TEST_DIR = Path(\"./test_data/cropdata\")  # test ë°ì´í„° ê²½ë¡œ\n",
    "\n",
    "# Submission\n",
    "OUTPUT_DIR = Path(\"./output\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)  # output í´ë” ì—†ìœ¼ë©´ ìƒì„±\n",
    "\n",
    "OUT_CSV = OUTPUT_DIR / \"baseline_submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".jfif\"}\n",
    "VIDEO_EXTS = {\".mp4\", \".mov\"}\n",
    "\n",
    "TARGET_SIZE = 224\n",
    "NUM_FRAMES = 10  # ë¹„ë””ì˜¤ ìƒ˜í”Œë§ í”„ë ˆì„ ìˆ˜\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_frame_indices(total_frames: int, num_frames: int) -> np.ndarray:\n",
    "    \"\"\"ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§\"\"\"\n",
    "    if total_frames <= 0:\n",
    "        return np.array([], dtype=int)\n",
    "    if total_frames <= num_frames:\n",
    "        return np.arange(total_frames, dtype=int)\n",
    "    return np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "\n",
    "def get_full_frame_padded(pil_img: Image.Image, target_size=(224, 224)) -> Image.Image:\n",
    "    \"\"\"ì „ì²´ ì´ë¯¸ì§€ë¥¼ ë¹„ìœ¨ ìœ ì§€í•˜ë©° ì •ì‚¬ê°í˜• íŒ¨ë”© ì²˜ë¦¬\"\"\"\n",
    "    img = pil_img.convert(\"RGB\")\n",
    "    img.thumbnail(target_size, Image.BICUBIC)\n",
    "    new_img = Image.new(\"RGB\", target_size, (0, 0, 0))\n",
    "    new_img.paste(img, ((target_size[0] - img.size[0]) // 2,\n",
    "                        (target_size[1] - img.size[1]) // 2))\n",
    "    return new_img\n",
    "\n",
    "def read_rgb_frames(file_path: Path, num_frames: int = NUM_FRAMES) -> List[np.ndarray]:\n",
    "    \"\"\"ì´ë¯¸ì§€ ë˜ëŠ” ë¹„ë””ì˜¤ì—ì„œ RGB í”„ë ˆì„ ì¶”ì¶œ\"\"\"\n",
    "    ext = file_path.suffix.lower()\n",
    "    \n",
    "    # ì´ë¯¸ì§€ íŒŒì¼\n",
    "    if ext in IMAGE_EXTS:\n",
    "        try:\n",
    "            img = Image.open(file_path).convert(\"RGB\")\n",
    "            return [np.array(img)]\n",
    "        except Exception:\n",
    "            return []\n",
    "    \n",
    "    # ë¹„ë””ì˜¤ íŒŒì¼\n",
    "    if ext in VIDEO_EXTS:\n",
    "        cap = cv2.VideoCapture(str(file_path))\n",
    "        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if total <= 0:\n",
    "            cap.release()\n",
    "            return []\n",
    "        \n",
    "        frame_indices = uniform_frame_indices(total, num_frames)\n",
    "        frames = []\n",
    "        \n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        cap.release()\n",
    "        return frames\n",
    "    \n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_face = mp.solutions.face_detection\n",
    "\n",
    "def _clamp(v, lo, hi):\n",
    "    return max(lo, min(hi, v))\n",
    "\n",
    "def crop_face_mediapipe(\n",
    "    img: Image.Image,\n",
    "    min_conf: float = 0.5,\n",
    "    margin: float = 0.25,     # ì–¼êµ´ ë°•ìŠ¤ ì£¼ë³€ ì—¬ìœ \n",
    "    make_square: bool = True,\n",
    "    fill_color=(0, 0, 0)      # ë°•ìŠ¤ê°€ ì´ë¯¸ì§€ ë°–ìœ¼ë¡œ ë‚˜ê°€ë©´ íŒ¨ë”© ìƒ‰\n",
    ") -> Image.Image:\n",
    "    \"\"\"\n",
    "    ì–¼êµ´ 1ê°œ(ê°€ì¥ í° ë°•ìŠ¤) í¬ë¡­. ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜.\n",
    "    \"\"\"\n",
    "    img = img.convert(\"RGB\")\n",
    "    w, h = img.size\n",
    "    rgb = np.array(img)  # RGB\n",
    "\n",
    "    # mediapipeëŠ” RGB ì…ë ¥ ì‚¬ìš©\n",
    "    with mp_face.FaceDetection(model_selection=0, min_detection_confidence=min_conf) as fd:\n",
    "        res = fd.process(rgb)\n",
    "\n",
    "    if not res.detections:\n",
    "        return img  # ì‹¤íŒ¨ ì‹œ ì›ë³¸\n",
    "\n",
    "    # ê°€ì¥ í° ì–¼êµ´ ì„ íƒ\n",
    "    best = None\n",
    "    best_area = -1\n",
    "    for det in res.detections:\n",
    "        bbox = det.location_data.relative_bounding_box\n",
    "        x1 = int(bbox.xmin * w)\n",
    "        y1 = int(bbox.ymin * h)\n",
    "        bw = int(bbox.width * w)\n",
    "        bh = int(bbox.height * h)\n",
    "        x2 = x1 + bw\n",
    "        y2 = y1 + bh\n",
    "        area = max(0, bw) * max(0, bh)\n",
    "        if area > best_area:\n",
    "            best_area = area\n",
    "            best = (x1, y1, x2, y2)\n",
    "\n",
    "    x1, y1, x2, y2 = best\n",
    "\n",
    "    # margin ì ìš©\n",
    "    bw = x2 - x1\n",
    "    bh = y2 - y1\n",
    "    mx = int(bw * margin)\n",
    "    my = int(bh * margin)\n",
    "    x1 -= mx; y1 -= my; x2 += mx; y2 += my\n",
    "\n",
    "    # ì •ì‚¬ê°í˜•ìœ¼ë¡œ í™•ì¥\n",
    "    if make_square:\n",
    "        cx = (x1 + x2) // 2\n",
    "        cy = (y1 + y2) // 2\n",
    "        half = max(x2 - x1, y2 - y1) // 2\n",
    "        x1, x2 = cx - half, cx + half\n",
    "        y1, y2 = cy - half, cy + half\n",
    "\n",
    "    # ì´ë¯¸ì§€ ë°–ìœ¼ë¡œ ë‚˜ê°€ë©´ íŒ¨ë”© í›„ í¬ë¡­\n",
    "    pad_l = max(0, -x1)\n",
    "    pad_t = max(0, -y1)\n",
    "    pad_r = max(0, x2 - w)\n",
    "    pad_b = max(0, y2 - h)\n",
    "\n",
    "    if pad_l or pad_t or pad_r or pad_b:\n",
    "        new_w = w + pad_l + pad_r\n",
    "        new_h = h + pad_t + pad_b\n",
    "        canvas = Image.new(\"RGB\", (new_w, new_h), fill_color)\n",
    "        canvas.paste(img, (pad_l, pad_t))\n",
    "        x1 += pad_l; x2 += pad_l\n",
    "        y1 += pad_t; y2 += pad_t\n",
    "        return canvas.crop((x1, y1, x2, y2))\n",
    "\n",
    "    # ê·¸ëƒ¥ í¬ë¡­\n",
    "    x1 = _clamp(x1, 0, w)\n",
    "    y1 = _clamp(y1, 0, h)\n",
    "    x2 = _clamp(x2, 0, w)\n",
    "    y2 = _clamp(y2, 0, h)\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return img\n",
    "\n",
    "    return img.crop((x1, y1, x2, y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ë¹„ë””ì˜¤ í”„ë˜ì„ ì ìˆ˜ ì§‘ê³„ ë°©ì‹ // max, mean, topkmean ì§€ì›\n",
    "def aggregate_frame_probs(\n",
    "    probs: torch.Tensor,\n",
    "    method: str = \"mean\",\n",
    "    topk: int = 3\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    probs: shape [T] (ê° í”„ë ˆì„ì˜ fake í™•ë¥ )\n",
    "    method: \"mean\" | \"max\" | \"topkmean\"\n",
    "    \"\"\"\n",
    "    if probs.numel() == 0:\n",
    "        return 0.0\n",
    "\n",
    "    method = method.lower()\n",
    "\n",
    "    if method == \"mean\":\n",
    "        return float(probs.mean().item())\n",
    "\n",
    "    if method == \"max\":\n",
    "        return float(probs.max().item())\n",
    "\n",
    "    if method in (\"topk\", \"topkmean\", \"top-k\", \"top-k-mean\"):\n",
    "        k = int(topk)\n",
    "        k = max(1, min(k, probs.numel()))\n",
    "        topk_vals = torch.topk(probs, k=k, largest=True).values\n",
    "        return float(topk_vals.mean().item())\n",
    "\n",
    "    raise ValueError(f\"Unknown method: {method}. Use 'mean', 'max', or 'topkmean'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "class PreprocessOutput:\n",
    "    def __init__(\n",
    "        self,\n",
    "        filename: str,\n",
    "        imgs: List[Image.Image],\n",
    "        error: Optional[str] = None\n",
    "    ):\n",
    "        self.filename = filename\n",
    "        self.imgs = imgs\n",
    "        self.error = error\n",
    "\n",
    "def preprocess_one(file_path: Path,transform: Callable,num_frames: int = NUM_FRAMES) -> PreprocessOutput:\n",
    "    \"\"\"\n",
    "    íŒŒì¼ í•˜ë‚˜ì— ëŒ€í•œ ì „ì²˜ë¦¬ ìˆ˜í–‰\n",
    "    \n",
    "    Args:\n",
    "        file_path: ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ\n",
    "        num_frames: ë¹„ë””ì˜¤ì—ì„œ ì¶”ì¶œí•  í”„ë ˆì„ ìˆ˜\n",
    "    \n",
    "    Returns:\n",
    "        PreprocessOutput ê°ì²´\n",
    "    \"\"\"\n",
    "    try:\n",
    "        frames = read_rgb_frames(file_path, num_frames=num_frames)\n",
    "              \n",
    "        imgs: List[Image.Image] = []\n",
    "        \n",
    "        for rgb in frames:\n",
    "            pil = Image.fromarray(rgb).convert(\"RGB\")\n",
    "            # ğŸ”¹ ì–¼êµ´ í¬ë¡­ ì‹œë„ (ì‹¤íŒ¨í•˜ë©´ ì›ë³¸ ë°˜í™˜)\n",
    "            pil = crop_face_mediapipe(pil, min_conf=0.5, margin=0.25)\n",
    "            x = transform(pil)     # âœ… ì—¬ê¸°ì„œ Swin ì „ì²˜ë¦¬ ì ìš© (Tensorë¡œ ë³€í™˜)\n",
    "            imgs.append(x)\n",
    "        \n",
    "        return PreprocessOutput(file_path.name, imgs, None)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return PreprocessOutput(file_path.name, [], str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded: prithivMLmods/Deep-Fake-Detector-v2-Model\n",
      "Model config: num_labels=2\n",
      "id2label: {0: 'Realism', 1: 'Deepfake'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model...\")\n",
    "\n",
    "model = SwinForImageClassification.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    num_labels=1,                 # âœ… binary (real/fake)\n",
    "    ignore_mismatched_sizes=True  # âœ… head í¬ê¸° ë‹¬ë¼ë„ ë¡œë“œ\n",
    ").to(DEVICE)\n",
    "\n",
    "model.train()   # ğŸ”´ íŒŒì¸íŠœë‹ì´ë¯€ë¡œ train()\n",
    "\n",
    "print(f\"Model loaded: {MODEL_ID}\")\n",
    "print(f\"num_labels = {model.config.num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done: test_data\\croptestdata\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "inp = Path(\"./test_data/cropdata\")     # ì—¬ê¸°ì— ì‚¬ì§„ë“¤ ë„£ê¸°\n",
    "out = Path(\"./test_data/croptestdata\")  # ê²°ê³¼ë¬¼ ì €ì¥ ê²½ë¡œ\n",
    "out.mkdir(exist_ok=True)\n",
    "\n",
    "for p in inp.glob(\"*\"):\n",
    "    if p.suffix.lower() not in [\".jpg\",\".jpeg\",\".png\",\".jfif\"]:\n",
    "        continue\n",
    "    img = Image.open(p).convert(\"RGB\")\n",
    "    face = crop_face_mediapipe(img, min_conf=0.5, margin=0.25)\n",
    "    face.save(out / p.name)\n",
    "\n",
    "print(\"done:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def infer_file_score(\n",
    "    file_path: Path,\n",
    "    model,\n",
    "    transform,\n",
    "    device: str,\n",
    "    agg: str = \"mean\",\n",
    "    topk: int = 3,\n",
    "    num_frames: int = NUM_FRAMES\n",
    ") -> float:\n",
    "    out = preprocess_one(file_path, transform=transform, num_frames=num_frames)\n",
    "\n",
    "    if out.error or not out.imgs:\n",
    "        return 0.0\n",
    "\n",
    "    xs = torch.stack(out.imgs, dim=0).to(device)  # [T,3,224,224]\n",
    "\n",
    "    model.eval()\n",
    "    outputs = model(xs)\n",
    "\n",
    "    # âœ… logits ì¶”ì¶œ (HF ImageClassifierOutput / tuple / tensor ëª¨ë‘ ëŒ€ì‘)\n",
    "    if hasattr(outputs, \"logits\"):\n",
    "        logits = outputs.logits\n",
    "    elif isinstance(outputs, (tuple, list)):\n",
    "        logits = outputs[0]\n",
    "    else:\n",
    "        logits = outputs  # torch.Tensor\n",
    "\n",
    "    logits = logits.squeeze()\n",
    "\n",
    "    # logits shape ì •ë¦¬: [T]\n",
    "    if logits.dim() == 0:\n",
    "        logits = logits.unsqueeze(0)\n",
    "    elif logits.dim() > 1:\n",
    "        logits = logits.view(-1)\n",
    "\n",
    "    probs = torch.sigmoid(logits).detach().cpu()  # [T]\n",
    "    return aggregate_frame_probs(probs, method=agg, topk=topk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5226365923881531 0.5984196662902832 0.5226365923881531\n",
      "0.5174304842948914 0.7963025569915771 0.7775881886482239\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fp = Path(\"./test_data/cropdata/real_0.jpg\")\n",
    "score_mean = infer_file_score(fp, model, val_transform, DEVICE, agg=\"mean\")\n",
    "score_max  = infer_file_score(fp, model, val_transform, DEVICE, agg=\"max\")\n",
    "score_top3 = infer_file_score(fp, model, val_transform, DEVICE, agg=\"topkmean\", topk=3)\n",
    "\n",
    "print(score_mean, score_max, score_top3)\n",
    "\n",
    "fp1 = Path(\"./test_data/cropdata/abarnvbtwb.mp4\")\n",
    "score_mean = infer_file_score(fp1, model, val_transform, DEVICE, agg=\"mean\", num_frames=10)\n",
    "score_max  = infer_file_score(fp1, model, val_transform, DEVICE, agg=\"max\", num_frames=10)\n",
    "score_top3 = infer_file_score(fp1, model, val_transform, DEVICE, agg=\"topkmean\", topk=3, num_frames=10)\n",
    "\n",
    "print(score_mean, score_max, score_top3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_fake_probs(pil_images: List[Image.Image]) -> List[float]:\n",
    "    if not pil_images:\n",
    "        return []\n",
    "\n",
    "    probs: List[float] = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        inputs = processor(images=pil_images, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(DEVICE, non_blocking=True) for k, v in inputs.items()}\n",
    "        logits = model(**inputs).logits\n",
    "        batch_probs = F.softmax(logits, dim=1)[:, 1]\n",
    "        probs.extend(batch_probs.cpu().tolist())\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data length: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "preprocess_one() missing 1 required positional argument: 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# ì „ì²˜ë¦¬ ë° ì¶”ë¡ \u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m tqdm(files, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 8\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# 1. ì—ëŸ¬ ë¡œê¹…\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39merror:\n",
      "\u001b[1;31mTypeError\u001b[0m: preprocess_one() missing 1 required positional argument: 'transform'"
     ]
    }
   ],
   "source": [
    "files = sorted([p for p in TEST_DIR.iterdir() if p.is_file()])\n",
    "print(f\"Test data length: {len(files)}\")\n",
    "\n",
    "results: Dict[str, float] = {}\n",
    "\n",
    "# ì „ì²˜ë¦¬ ë° ì¶”ë¡ \n",
    "for file_path in tqdm(files, desc=\"Processing\"):\n",
    "    out = preprocess_one(file_path)\n",
    "    \n",
    "    # 1. ì—ëŸ¬ ë¡œê¹…\n",
    "    if out.error:\n",
    "        print(f\"[WARN] {out.filename}: {out.error}\")\n",
    "    \n",
    "    # 2. ì •ìƒ ì¶”ë¡ \n",
    "    elif out.imgs:\n",
    "        probs = infer_fake_probs(out.imgs)\n",
    "        results[out.filename] = float(np.mean(probs)) if probs else 0.0\n",
    "    \n",
    "    # 3. ë‘˜ ë‹¤ ì—†ìœ¼ë©´ 0.0 (real)\n",
    "    else:\n",
    "        results[out.filename] = 0.0\n",
    "\n",
    "print(f\"Inference completed. Processed: {len(results)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]c:\\Users\\Kim\\HAI\\.venv\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: output\\baseline_submission.csv\n",
      "         filename      mean       max      top3\n",
      "0  abarnvbtwb.mp4  0.517430  0.796303  0.777588\n",
      "1      real_0.jpg  0.522637  0.598420  0.522637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "AGG_METHOD = \"topkmean\"   # \"mean\" | \"max\" | \"topkmean\"\n",
    "TOPK = 3\n",
    "\n",
    "files = sorted([p for p in TEST_DIR.rglob(\"*\") if p.is_file()])\n",
    "rows = []\n",
    "\n",
    "for fp in tqdm(files, desc=\"Inference\"):\n",
    "    if fp.suffix.lower() not in (IMAGE_EXTS | VIDEO_EXTS):\n",
    "        continue\n",
    "\n",
    "    prob = infer_file_score(\n",
    "        fp, model, val_transform, DEVICE,\n",
    "        agg=AGG_METHOD, topk=TOPK, num_frames=NUM_FRAMES\n",
    "    )\n",
    "\n",
    "    # í˜¹ì‹œ ëª¨ë¥¼ ì•ˆì „ì¥ì¹˜ (0~1ë¡œ í´ë¨í”„)\n",
    "    prob = float(max(0.0, min(1.0, prob)))\n",
    "\n",
    "    rows.append({\"filename\": fp.name, \"prob\": prob})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print(\"saved:\", OUT_CSV)\n",
    "print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
